import torch
from torch import nn
import torch.optim as optim
import torch.nn.functional as F
import math
import numpy as np
from config import parameters as conf
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

if conf.pretrained_model == "bert":
    from transformers import BertModel
elif conf.pretrained_model == "roberta":
    from transformers import RobertaModel
elif conf.pretrained_model == "finbert":
    from transformers import BertModel
elif conf.pretrained_model == "longformer":
    from transformers import LongformerModel


class Bert_model(nn.Module):

    def __init__(self, num_decoder_layers, hidden_size, dropout_rate, input_length,
                 program_length, op_list, const_list, num_char_length, num_emb_dim):

        super(Bert_model, self).__init__()

        self.op_list_size = len(op_list)
        self.const_list_size = len(const_list)
        self.reserved_token_size = self.op_list_size + self.const_list_size
        self.program_length = program_length
        self.hidden_size = hidden_size
        self.const_list = const_list
        self.op_list = op_list
        self.input_length = input_length
        self.num_char_length = num_char_length
        self.num_emb_dim = num_emb_dim

        self.reserved_ind = nn.Parameter(torch.arange(
            0, self.reserved_token_size), requires_grad=False)
        self.reserved_go = nn.Parameter(torch.arange(op_list.index(
            'GO'), op_list.index('GO') + 1), requires_grad=False)

        self.reserved_para = nn.Parameter(torch.arange(op_list.index(
            ')'), op_list.index(')') + 1), requires_grad=False)

        # masking for decoidng for test time
        op_ones = nn.Parameter(torch.ones(
            self.op_list_size), requires_grad=False)
        op_zeros = nn.Parameter(torch.zeros(
            self.op_list_size), requires_grad=False)
        other_ones = nn.Parameter(torch.ones(
            input_length + self.const_list_size), requires_grad=False)
        other_zeros = nn.Parameter(torch.zeros(
            input_length + self.const_list_size), requires_grad=False)
        self.op_only_mask = nn.Parameter(
            torch.cat((op_ones, other_zeros), 0), requires_grad=False)
        self.seq_only_mask = nn.Parameter(
            torch.cat((op_zeros, other_ones), 0), requires_grad=False)

        # for ")"
        para_before_ones = nn.Parameter(torch.ones(
            op_list.index(')')), requires_grad=False)
        para_after_ones = nn.Parameter(torch.ones(
            input_length + self.reserved_token_size - op_list.index(')') - 1), requires_grad=False)
        para_zero = nn.Parameter(torch.zeros(1), requires_grad=False)
        self.para_mask = nn.Parameter(torch.cat(
            (para_before_ones, para_zero, para_after_ones), 0), requires_grad=False)

        # for step embedding
        # self.step_masks = []
        all_tmp_list = self.op_list + self.const_list
        self.step_masks = nn.Parameter(torch.zeros(
            conf.max_step_ind, input_length + self.reserved_token_size), requires_grad=False)
        for i in range(conf.max_step_ind):
            this_step_mask_ind = all_tmp_list.index("#" + str(i))
            self.step_masks[i, this_step_mask_ind] = 1.0

        # self.step_mask_eye = torch.eye(conf.max_step_ind)

        if conf.pretrained_model == "bert":
            self.bert = BertModel.from_pretrained(
                conf.model_size, cache_dir=conf.cache_dir)
        elif conf.pretrained_model == "roberta":
            self.bert = RobertaModel.from_pretrained(
                conf.model_size, cache_dir=conf.cache_dir)
        elif conf.pretrained_model == "finbert":
            self.bert = BertModel.from_pretrained(
                conf.model_size, cache_dir=conf.cache_dir)
        elif conf.pretrained_model == "longformer":
            self.bert = LongformerModel.from_pretrained(
                conf.model_size, cache_dir=conf.cache_dir)

        self.cls_prj = nn.Linear(hidden_size, hidden_size, bias=True)
        self.cls_dropout = nn.Dropout(dropout_rate)

        self.seq_prj = nn.Linear(hidden_size, hidden_size, bias=True)
        self.seq_dropout = nn.Dropout(dropout_rate)

        self.reserved_token_embedding = nn.Embedding(
            self.reserved_token_size, hidden_size)

        self.num_char_embedding = nn.Embedding(self.num_char_length, num_emb_dim)
        # attentions
        self.decoder_history_attn_prj = nn.Linear(
            hidden_size, hidden_size, bias=True)
        self.decoder_history_attn_dropout = nn.Dropout(dropout_rate)

        self.question_attn_prj = nn.Linear(hidden_size, hidden_size, bias=True)
        self.question_attn_dropout = nn.Dropout(dropout_rate)

        self.question_summary_attn_prj = nn.Linear(
            hidden_size, hidden_size, bias=True)
        self.question_summary_attn_dropout = nn.Dropout(dropout_rate)

        if conf.sep_attention:
            self.input_embeddings_prj = nn.Linear(
                hidden_size*3, hidden_size, bias=True)
        else:
            self.input_embeddings_prj = nn.Linear(
                hidden_size*2, hidden_size, bias=True)
        self.input_embeddings_layernorm = nn.LayerNorm([1, hidden_size])

        self.option_embeddings_prj = nn.Linear(
            hidden_size*2, hidden_size, bias=True)

        # decoder lstm
        self.rnn = torch.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,
                                 num_layers=conf.num_decoder_layers, batch_first=True)

        # num char encoder
        
        self.num_bilstm = torch.nn.LSTM(input_size=num_emb_dim, hidden_size=hidden_size // 2,
                                        num_layers=conf.num_encoder_layers, bidirectional=True)
        self.num_char_prj = nn.Linear(hidden_size, hidden_size, bias=True)
        self.num_char_dropout = nn.Dropout(dropout_rate)

        # num attention
        self.num_attn_prj = nn.Linear(hidden_size, hidden_size, bias=True)
        self.num_attn_dropout = nn.Dropout(dropout_rate)

        # seq_out_prj
        self.seqout_prj = nn.Linear(hidden_size * 2, hidden_size, bias=True)
        self.seqout_dropout = nn.Dropout(dropout_rate)

        
        # step vector
        self.decoder_step_proj = nn.Linear(
            3*hidden_size, hidden_size, bias=True)
        self.decoder_step_proj_dropout = nn.Dropout(dropout_rate)

        self.step_mix_proj = nn.Linear(
            hidden_size*2, hidden_size, bias=True)

    def forward(self, is_training, input_ids, input_mask, segment_ids, option_mask, program_ids, program_mask, num_char_ids, number_mask, num_char_mask,  device):

        bert_outputs = self.bert(
            input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids)
        # print("="*30)
        # print("input_ids.size(), ", input_ids.size())     # [batch, seq_length], [16, 512]
        # print("number_mask: ", number_mask.size())
        # print("input_mask.size(), ", input_mask.size())   # [batch, seq_length], [16, 512]
        # print("segment_ids.size(), ", segment_ids.size()) # [batch, seq_length], [16, 512]
        # print("option_mask.size()", option_mask.size())   # [batch, option_length], [16, 556]
        # print("program_ids.size()", program_ids.size())   # [batch, program_length], [16, 30]
        # print("program_mask.size()", program_mask.size()) # [batch, program_length], [16, 30]

        ######
        # Step 1: get the sequence, including questions and retrieved text: {h_i^e}
        ######
        bert_sequence_output = bert_outputs.last_hidden_state  # [batch, seq_length, hidden], [16, 512, 768]
        bert_pooled_output = bert_sequence_output[:, 0, :]  # [batch, hidden], [16, 768]
        
        batch_size, seq_length, bert_dim = list(bert_sequence_output.size())
        
        pooled_output = self.cls_prj(bert_pooled_output)  # if conf.sep_attention is True, the pooled_output will not be used
        pooled_output = self.cls_dropout(pooled_output)

        option_size = self.reserved_token_size + seq_length  # 556

        sequence_output = self.seq_prj(bert_sequence_output)
        sequence_output = self.seq_dropout(sequence_output)  # [batch_size, seq_length, hidden], [16, 512, 768]

        ######
        # Step new1: get number embeddings and number_hidden_state
        # the number char_hidden_avg will be concated with the input sequence
        # therefore, for the word (not number), we copy the output of the encoder here
        ######
        num_char_embeddings = self.num_char_embedding(num_char_ids)
        size_a, size_b, size_c, size_d = num_char_embeddings.size()
        num_char_embeddings = num_char_embeddings.reshape(-1, size_c, size_d)   # [16 * 512, 10, 300]
        # add pad, get bilstm output
        num_char_length = num_char_mask.sum(-1).reshape(-1)  # [16 * 512]
        num_char_length += (num_char_length == 0).long()  # add 1 to those has 0 number, we can multiply 0 again to avoid the calculation
        num_char_length = num_char_length.tolist()
        num_char_embeddings_pad = torch.nn.utils.rnn.pack_padded_sequence(input=num_char_embeddings, lengths=num_char_length, batch_first=True, enforce_sorted=False)       
        num_char_hidden, _  = self.num_bilstm(num_char_embeddings_pad)
        num_char_hidden, out_len = pad_packed_sequence(num_char_hidden, batch_first=True)
        num_char_hidden = num_char_hidden.reshape(size_a, size_b, size_c, -1) # because bilstm
        num_char_mask_repeat = num_char_mask.unsqueeze(-1).repeat(1,1,1,self.hidden_size) # batch, seq_length, max_num_length, hidden
        
        num_char_hidden = num_char_hidden * num_char_mask_repeat  # same as above
        num_char_hidden_sum = num_char_hidden.sum(-2)
        num_char_mask = num_char_mask.sum(-1).unsqueeze(-1).repeat(1,1,self.hidden_size) + 1e-7
        num_char_hidden_avg = num_char_hidden_sum / num_char_mask  # batch, seq_length, hidden
        num_char_output = self.num_char_prj(num_char_hidden_avg)
        num_char_output = self.num_char_dropout(num_char_output)  # batch, seq_length, hidden

        mask = number_mask.unsqueeze(-1).repeat(1,1,self.hidden_size) # batch, seq_length, hidden
        
        concat_num_word_output = num_char_output * mask + sequence_output * (mask - 1)  # batch, seq_length, hidden
        # copy the output of the encoder here
        # number_mask: [batch, seq_length]
        
        num_attn_vec = self.num_attn_prj(concat_num_word_output)
        num_attn_vec = self.num_attn_dropout(num_attn_vec)  # batch, seq_length, hidden
        # print("num_attn_vec: ", num_attn_vec.size())
        
        num_attn_w = torch.matmul(concat_num_word_output, torch.transpose(num_attn_vec, 1, 2))  # batch, seq_length, seq_length (len_generated)
        # print("num_attn_w: ", num_attn_w.size())
       #  print("mask: ", mask.size())
        attn_mask = number_mask.unsqueeze(-1).repeat(1, 1, num_attn_w.size()[-1])  # batch, 
        num_attn_w -= 1e6 * (1 - attn_mask)
        num_attn_w = F.softmax(num_attn_w, dim=1)
        # print("after softmax, num_attn_w.size(): ", num_attn_w.size())  # 
        num_ctx_out = torch.matmul(
            torch.transpose(num_attn_w, 1, 2), concat_num_word_output)  # batch, seq_length, hidden    
        # print("num_ctx_out: ", num_ctx_out.size())  # batch, seq_length, hidden

        sequence_output = torch.cat([sequence_output, num_ctx_out], dim=-1)
        sequence_output = self.seqout_prj(sequence_output)
        sequence_output = self.seqout_dropout(sequence_output)
        # print(sequence_output)

        
        ######
        # Step 2: get option embeddings: {h_i^s, h_i^m}
        #         and concat it with sequence_output: H
        ######
        
        op_embeddings = self.reserved_token_embedding(self.reserved_ind)
        op_embeddings = op_embeddings.repeat(batch_size, 1, 1)  # [batch_size, reserved_ind_length, hidden], [16, 44, 768], the length of reserved_ind = len(op_list) + len(const_list)

        # [batch, op + seq len, hidden]
        initial_option_embeddings = torch.cat([op_embeddings, sequence_output], dim=1)

        ######
        # Step 3: init something used for LSTM decoder
        ######
        
        # for init, only one symbol "GO", so the size of decoder_output is [batch_size, 1, hidden]
        init_decoder_output = self.reserved_token_embedding(self.reserved_go)  # [1, 768]

        decoder_output = init_decoder_output.repeat(batch_size, 1, 1)  # [16, 1, 768]
        
        if conf.sep_attention:
            decoder_history = decoder_output
        else:
            decoder_history = torch.unsqueeze(pooled_output, dim=-1)

        # initialize the hidden state for the LSTM decoder
        decoder_state_h = torch.zeros(1, batch_size, self.hidden_size, device=device)
        decoder_state_c = torch.zeros(1, batch_size, self.hidden_size, device=device)

        ######
        # Step 4: prepare something for future use
        ######
        
        split_program_ids = torch.split(program_ids, 1, dim=1)  # len(split_program_ids) = 30, split_program_ids[0].size() = [16, 1]

        # What's the float_input_mask for?
        float_input_mask = input_mask.float()
        float_input_mask = torch.unsqueeze(float_input_mask, dim=-1)

        # used for updating option embeddings, adding step embedding
        this_step_new_op_emb = initial_option_embeddings  # [batch, option_length, hidden]

        logits = []

        ######
        # Step 5: generate program
        ######
        
        for cur_step in range(self.program_length):

            ######
            # Step 5.1: get decoder history attention: att_h
            ######
            decoder_history_attn_vec = self.decoder_history_attn_prj(decoder_output)  # [batch, 1, hidden], [16, 1, 768]
            decoder_history_attn_vec = self.decoder_history_attn_dropout(decoder_history_attn_vec)
            decoder_history_attn_w = torch.matmul(
                decoder_history, torch.transpose(decoder_history_attn_vec, 1, 2))  # [batch, cur_step + 1, 1]
            decoder_history_attn_w = F.softmax(decoder_history_attn_w, dim=1)  # [batch, cur_step + 1, 1]
            decoder_history_ctx_embeddings = torch.matmul(
                torch.transpose(decoder_history_attn_w, 1, 2), decoder_history)  # [batch, 1, hidden],[16, 1, 768]

            ######
            # Step 5.2: get attention for input sequence: att_p
            ######
            if conf.sep_attention:
                # input seq att
                question_attn_vec = self.question_attn_prj(decoder_output)
                question_attn_vec = self.question_attn_dropout(question_attn_vec) #[batch, 1, hidden],[16, 1, 768]
                question_attn_w = torch.matmul(
                    sequence_output, torch.transpose(question_attn_vec, 1, 2))#[batch, seq_length, 1],[16, 512, 1]
                question_attn_w -= 1e6 * (1 - float_input_mask)
                question_attn_w = F.softmax(question_attn_w, dim=1)  # [batch, seq_length, 1], [16, 512, 1]
                question_ctx_embeddings = torch.matmul(
                    torch.transpose(question_attn_w, 1, 2), sequence_output)  # [batch, 1, hidden], [16, 1, 768]

            ######    
            # Step 5.3: get  another input sequence attention: att_p'
            ######
            question_summary_vec = self.question_summary_attn_prj(decoder_output)
            question_summary_vec = self.question_summary_attn_dropout(question_summary_vec)  # [batch, 1, hidden]
            question_summary_w = torch.matmul(
                sequence_output, torch.transpose(question_summary_vec, 1, 2)) #[batch, seq_length, 1],[16, 512, 1]
            question_summary_w -= 1e6 * (1 - float_input_mask)
            question_summary_w = F.softmax(question_summary_w, dim=1)   # [batch, seq_length, 1], [16, 512, 1]
            question_summary_embeddings = torch.matmul(
                torch.transpose(question_summary_w, 1, 2), sequence_output)

            ######
            # Step 5.4: get contextual information C_T
            ######
            if conf.sep_attention:
                # [batch, 1, hidden * 3], [16, 1, 2304]
                concat_input_embeddings = torch.cat([decoder_history_ctx_embeddings,
                                                     question_ctx_embeddings,
                                                     decoder_output], dim=-1)  
            else:
                concat_input_embeddings = torch.cat([decoder_history_ctx_embeddings,
                                                     decoder_output], dim=-1)
            input_embeddings = self.input_embeddings_prj(concat_input_embeddings) #[batch, 1, hidden],[16, 1, 768]
            if conf.layer_norm:
                input_embeddings = self.input_embeddings_layernorm(
                    input_embeddings)

            ######
            # Step 5.5: get all token embeddings: H_T'
            ######
            question_option_vec = this_step_new_op_emb * question_summary_embeddings  # [batch, option_size, hidden], [16 556, 768]
            option_embeddings = torch.cat(
                [this_step_new_op_emb, question_option_vec], dim=-1)  # [batch, option_size, hidden*2], [16, 556, 1536]           
            option_embeddings = self.option_embeddings_prj(option_embeddings)  # [batch, option_size, hidden], [16, 556, 768]
           
            ######
            # Step 5.6: get logits
            ######
            option_logits = torch.matmul(
                option_embeddings, torch.transpose(input_embeddings, 1, 2)) # batch, option_size, 1],[16, 556, 1]
            option_logits = torch.squeeze(option_logits, dim=2) # [batch, op + seq_len],op + seq_len = option_size
            option_logits -= 1e6 * (1 - option_mask)
            logits.append(option_logits)

            ######
            # Step 6: update state
            ######
            if is_training:
                program_index = torch.unsqueeze(split_program_ids[cur_step], dim=1)  # [batch, 1, 1], [16, 1, 1]
                
            else:
                # constrain decoding
                if cur_step % 4 == 0 or (cur_step + 1) % 4 == 0:
                    # op round
                    option_logits -= 1e6 * self.seq_only_mask
                else:
                    # number round
                    option_logits -= 1e6 * self.op_only_mask

                if (cur_step + 1) % 4 == 0:
                    # ")" round
                    option_logits -= 1e6 * self.para_mask
                    # print(program_index)

                program_index = torch.argmax(option_logits, axis=-1, keepdim=True)
                program_index = torch.unsqueeze(program_index, dim=1)

            if (cur_step + 1) % 4 == 0:
                
                # update op embeddings
                this_step_index = cur_step // 4
                this_step_list_index = (
                    self.op_list + self.const_list).index("#" + str(this_step_index)) # ??? integer
                this_step_mask = self.step_masks[this_step_index, :]  # [option_size], [556]
                decoder_step_vec = self.decoder_step_proj(concat_input_embeddings)
                decoder_step_vec = self.decoder_step_proj_dropout(decoder_step_vec)#[batch,1,hidden], [16, 1, 768]
                decoder_step_vec = torch.squeeze(decoder_step_vec)  # [batch, hidden], [16, 768]

                this_step_new_emb = decoder_step_vec  # [batch, hidden]
                this_step_new_emb = torch.unsqueeze(this_step_new_emb, 1)
                this_step_new_emb = this_step_new_emb.repeat(
                    1, self.reserved_token_size+self.input_length, 1)  # [batch, op seq, hidden]
                this_step_mask = torch.unsqueeze(this_step_mask, 0)  # [1, op seq]
                this_step_mask = torch.unsqueeze(this_step_mask, 2)  # [1, op seq, 1]
                this_step_mask = this_step_mask.repeat(batch_size, 1, self.hidden_size)  # [batch, op seq, hidden]
                this_step_new_op_emb = torch.where(
                    this_step_mask > 0, this_step_new_emb, initial_option_embeddings)

            program_index = torch.repeat_interleave(program_index, self.hidden_size, dim=2)  # [batch, 1, hidden]
            input_program_embeddings = torch.gather(option_embeddings, dim=1, index=program_index)
            
            decoder_output, (decoder_state_h, decoder_state_c) = self.rnn(
                input_program_embeddings, (decoder_state_h, decoder_state_c))
            decoder_history = torch.cat(
                [decoder_history, input_program_embeddings], dim=1)  # [batch, cur_step + 1, hidden]

        logits = torch.stack(logits, dim=1)
        
        return logits
